{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection: Pre-deep learning approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Introduction to the Script\n",
    "2. Preparing Your Data\n",
    "3. Model Warmup for Classification and Regression\n",
    "4. Visualizing Model Performance\n",
    "5. Conclusion\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Introduction to the Script\n",
    "\n",
    "This script is designed to assist users in the preliminary steps of machine learning model selection and evaluation. It provides functions for model warmup, which involves training and tuning several machine learning models using grid search cross-validation. The script supports both classification and regression tasks. \n",
    "Many times, before stepping into deep learning, we process our data and still don't know what model to use. Whether our problem is a regression one or a classification one, we have a variety of models to choose from such as SVC, SVR, Decision Trees, and more.\n",
    "\n",
    "### What models does this script support\n",
    "\n",
    "- DecisionTreeClassifier (see sklearn documentation of DTC) [link](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "- DecisionTreeRegressor (see sklearn documentation of DTR) [link](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n",
    "- GradientBoostingClassifier (see sklearn documentation of GBC) [link](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
    "- GradientBoostRegressor (see sklearn documentation of GBR) [link](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)\n",
    "- RandomForestClassifier (see sklearn documentation of RFC) [link](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "- RandomForestRegressor (see sklearn documentation of RFR) [link](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "- SVC (see sklearn documentation of SVC) [link](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "- SVR (see sklearn documentation of SVR) [link](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n",
    "- LogisticRegression (see sklearn documentation of LogisticRegression) [link](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "- LinearRegression (see documentation of LinearRegression) [link](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "- Ridge (see sklearn documentation of Ridge) [link](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n",
    "\n",
    "\n",
    "### 2. Preparing Your Data\n",
    "\n",
    "Before using the script, ensure that your data is properly preprocessed and split into training and validation sets. The `X_train`, `y_train`, `X_val`, and `y_val` variables should contain the features and target variables for training and validation.\n",
    "\n",
    "### 3. Model Warmup for Classification and Regression\n",
    "\n",
    "#### Classification:\n",
    "To warm up classification models, call the `warmup_classification()` function with your training data. This function performs grid search cross-validation to find the best hyperparameters for logistic regression, decision tree, gradient boosting, random forest, and support vector machine (SVM) classifiers.\n",
    "\n",
    "#### Regression:\n",
    "To warm up regression models, call the `warmup_regression()` function with your training data. Similar to classification, this function performs grid search cross-validation to find the best hyperparameters for linear regression, decision tree, gradient boosting, random forest, and support vector regression (SVR) models.\n",
    "\n",
    "### 4. Visualizing Model Performance\n",
    "\n",
    "After warming up the models, you can visualize their performance on both the training and validation sets. The `visualization_class()` function is used for classification models, while the `visualization_regress()` function is used for regression models. These functions display the accuracy or R-squared scores of each model on the training and validation sets.\n",
    "\n",
    "### 5. Conclusion\n",
    "\n",
    "Using this script, you can efficiently explore various machine learning models, tune their hyperparameters, and evaluate their performance on your dataset. Experiment with different models and hyperparameters to find the best model for your specific task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_classification(X_train, y_train, multiclass=False):\n",
    "    # Logistic regression\n",
    "    param_grid_logistic = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    }\n",
    "    log_r = LogisticRegression(random_state=42)\n",
    "    if multiclass == True :\n",
    "        log_r = LogisticRegression(multi_class='multinomial', random_state=42)\n",
    "    grid_search_logistic = GridSearchCV(\n",
    "        log_r, \n",
    "        param_grid= param_grid_logistic, \n",
    "        cv=5, \n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    grid_search_logistic.fit(X_train, y_train)\n",
    "    best_model_logistic = grid_search_logistic.best_estimator_\n",
    "\n",
    "    # Decision tree\n",
    "    param_grid_dt = {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': range(1,20),\n",
    "        'min_samples_split': range(2,21),\n",
    "        'min_samples_leaf': range(1,21)\n",
    "    }\n",
    "    grid_search_dt = GridSearchCV(\n",
    "        DecisionTreeClassifier(random_state=42),\n",
    "        param_grid= param_grid_dt,\n",
    "        cv = 5,\n",
    "        scoring = 'accuracy',\n",
    "        n_jobs= -1\n",
    "    )\n",
    "    grid_search_dt.fit(X_train, y_train) \n",
    "    best_model_dt = grid_search_dt.best_estimator_\n",
    "    \n",
    "    # Gradient Boosting\n",
    "    param_grid_gb = {\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'min_samples_split': [2, 4],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }\n",
    "    grid_search_gb = GridSearchCV(\n",
    "        GradientBoostingClassifier(random_state=42),\n",
    "        param_grid= param_grid_gb,\n",
    "        cv= 5,\n",
    "        n_jobs=-1,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    grid_search_gb.fit(X_train, y_train)\n",
    "    best_model_gb = grid_search_gb.best_estimator_\n",
    "\n",
    "    #Random forest\n",
    "    param_grid_rf = {\n",
    "        'n_estimators': [100, 200, 300],  \n",
    "        'max_depth': [None, 10, 20, 30],  \n",
    "        'min_samples_split': [2, 5, 10],  \n",
    "        'min_samples_leaf': [1, 2, 4],    \n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "    grid_search_rf = GridSearchCV(\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        param_grid= param_grid_rf,\n",
    "        cv=5,\n",
    "        verbose=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search_rf.fit(X_train, y_train)\n",
    "    best_model_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "    # SVM\n",
    "    param_grid_svm = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "        'kernel': ['rbf', 'poly', 'sigmoid', 'linear']\n",
    "    }\n",
    "    svm = SVC(random_state=42)\n",
    "    if multiclass == True:\n",
    "        svm = SVC(random_state=42,decision_function_shape='ovr')\n",
    "    grid_search_svm = GridSearchCV(\n",
    "        svm, \n",
    "        param_grid_svm, \n",
    "        refit=True, \n",
    "        verbose=3, \n",
    "        cv=5, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search_svm.fit(X_train, y_train)\n",
    "    best_model_svm = grid_search_svm.best_estimator_\n",
    "\n",
    "    return best_model_logistic, best_model_dt, best_model_gb, best_model_rf, best_model_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_regression(X_train, y_train):\n",
    "    # Linear Regression \n",
    "    param_grid_linear = {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    }\n",
    "    grid_search_linear = GridSearchCV(\n",
    "        Ridge(random_state=42), \n",
    "        param_grid=param_grid_linear, \n",
    "        cv=5, \n",
    "        scoring='neg_mean_squared_error'\n",
    "    )\n",
    "    grid_search_linear.fit(X_train, y_train)\n",
    "    best_model_linear = grid_search_linear.best_estimator_\n",
    "\n",
    "    # Decision Tree Regressor\n",
    "    param_grid_dt = {\n",
    "        'criterion': ['mse', 'friedman_mse', 'mae'],\n",
    "        'max_depth': range(1,20),\n",
    "        'min_samples_split': range(2,21),\n",
    "        'min_samples_leaf': range(1,21)\n",
    "    }\n",
    "    grid_search_dt = GridSearchCV(\n",
    "        DecisionTreeRegressor(random_state=42),\n",
    "        param_grid=param_grid_dt,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search_dt.fit(X_train, y_train)\n",
    "    best_model_dt = grid_search_dt.best_estimator_\n",
    "    \n",
    "    # Gradient Boosting Regressor\n",
    "    param_grid_gb = {\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'min_samples_split': [2, 4],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }\n",
    "    grid_search_gb = GridSearchCV(\n",
    "        GradientBoostingRegressor(random_state=42),\n",
    "        param_grid=param_grid_gb,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search_gb.fit(X_train, y_train)\n",
    "    best_model_gb = grid_search_gb.best_estimator_\n",
    "\n",
    "    # Random Forest Regressor\n",
    "    param_grid_rf = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'min_samples_split': [2, 4],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }\n",
    "    grid_search_rf = GridSearchCV(\n",
    "        RandomForestRegressor(random_state=42),\n",
    "        param_grid=param_grid_rf,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search_rf.fit(X_train, y_train)\n",
    "    best_model_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "    # SVR\n",
    "    param_grid_svr = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'kernel': ['rbf', 'poly', 'sigmoid', 'linear']\n",
    "    }\n",
    "    grid_search_svr = GridSearchCV(\n",
    "        SVR(),\n",
    "        param_grid=param_grid_svr,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search_svr.fit(X_train, y_train)\n",
    "    best_model_svr = grid_search_svr.best_estimator_\n",
    "\n",
    "    return best_model_linear, best_model_dt, best_model_gb, best_model_rf, best_model_svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup(X_train, y_train, problem = 'binary_classifcation'):\n",
    "    if problem == 'binary_classification':\n",
    "        warmup_classification(X_train, y_train)\n",
    "    elif problem == 'multiclass_classification':\n",
    "        warmup_classification(X_train, y_train, False)\n",
    "    else: \n",
    "        warmup_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  warmup_regression(X_train, y_train)\n",
    "def visualization_class(logistic, decision_tree, gradient_boost, random_forest, svm, X_train, y_train, X_val, y_val):\n",
    "    predictions_train = logistic.predict(X_train)\n",
    "    predictions_val = logistic.predict(X_val)\n",
    "    logistic_train = accuracy_score(predictions_train,y_train)\n",
    "    logistic_val = accuracy_score(predictions_val, y_val)\n",
    "    print(\"\\033[1m\" + \"Logistic Regression\" + \"\\033[0m\"+\":\")\n",
    "    print('train: ',logistic_train*100,'%')\n",
    "    print('val: ',logistic_val*100,'%')\n",
    "\n",
    "    predictions_train = decision_tree.predict(X_train)\n",
    "    predictions_val = decision_tree.predict(X_val)\n",
    "    dt_train = accuracy_score(predictions_train,y_train)\n",
    "    dt_val = accuracy_score(predictions_val, y_val)\n",
    "    print(\"\\033[1m\" + \"Decision Tree\" + \"\\033[0m\"+\":\")\n",
    "    print('train: ',dt_train*100,'%')\n",
    "    print('val: ',dt_val*100,'%')\n",
    "\n",
    "    predictions_train = gradient_boost.predict(X_train)\n",
    "    predictions_val = gradient_boost.predict(X_val)\n",
    "    gb_train = accuracy_score(predictions_train,y_train)\n",
    "    gb_val = accuracy_score(predictions_val, y_val)\n",
    "    print(\"\\033[1m\" + \"Gradient Boost\" + \"\\033[0m\"+\":\")\n",
    "    print('train: ',gb_train*100,'%')\n",
    "    print('val: ',gb_val*100,'%')\n",
    "\n",
    "    predictions_train = random_forest.predict(X_train)\n",
    "    predictions_val = random_forest.predict(X_val)\n",
    "    rf_train = accuracy_score(predictions_train,y_train)\n",
    "    rf_val = accuracy_score(predictions_val, y_val)\n",
    "    print(\"\\033[1m\" + \"Random Forest\" + \"\\033[0m\"+\":\")\n",
    "    print('train: ',rf_train*100,'%')\n",
    "    print('val: ',rf_val*100,'%')\n",
    "\n",
    "    predictions_train = svm.predict(X_train)\n",
    "    predictions_val = svm.predict(X_val)\n",
    "    svm_train = accuracy_score(predictions_train,y_train)\n",
    "    svm_val = accuracy_score(predictions_val, y_val)\n",
    "    print(\"\\033[1m\" + \"SVM\" + \"\\033[0m\"+\":\")\n",
    "    print('train: ',svm_train*100,'%')\n",
    "    print('val: ',svm_val*100,'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization_regress(linear, decision_tree, gradient_boost, random_forest, svr, X_train, y_train, X_val, y_val):\n",
    "    predictions_train = linear.predict(X_train)\n",
    "    predictions_val = linear.predict(X_val)\n",
    "    linear_train = r2_score(y_train, predictions_train)\n",
    "    linear_val = r2_score(y_val, predictions_val)\n",
    "    print(\"\\033[1m\" + \"Linear Regression\" + \"\\033[0m\" + \":\")\n",
    "    print('train: ', linear_train*100, '%')\n",
    "    print('val: ', linear_val*100, '%')\n",
    "    \n",
    "    predictions_train = decision_tree.predict(X_train)\n",
    "    predictions_val = decision_tree.predict(X_val)\n",
    "    dt_train = r2_score(y_train, predictions_train)\n",
    "    dt_val = r2_score(y_val, predictions_val)\n",
    "    print(\"\\033[1m\" + \"Decision Tree\" + \"\\033[0m\" + \":\")\n",
    "    print('train: ', dt_train*100, '%')\n",
    "    print('val: ', dt_val*100, '%')\n",
    "    \n",
    "    predictions_train = gradient_boost.predict(X_train)\n",
    "    predictions_val = gradient_boost.predict(X_val)\n",
    "    gb_train = r2_score(y_train, predictions_train)\n",
    "    gb_val = r2_score(y_val, predictions_val)\n",
    "    print(\"\\033[1m\" + \"Gradient Boost\" + \"\\033[0m\" + \":\")\n",
    "    print('train: ', gb_train*100, '%')\n",
    "    print('val: ', gb_val*100, '%')\n",
    "    \n",
    "    predictions_train = random_forest.predict(X_train)\n",
    "    predictions_val = random_forest.predict(X_val)\n",
    "    rf_train = r2_score(y_train, predictions_train)\n",
    "    rf_val = r2_score(y_val, predictions_val)\n",
    "    print(\"\\033[1m\" + \"Random Forest\" + \"\\033[0m\" + \":\")\n",
    "    print('train: ', rf_train*100, '%')\n",
    "    print('val: ', rf_val*100, '%')\n",
    "    \n",
    "    predictions_train = svr.predict(X_train)\n",
    "    predictions_val = svr.predict(X_val)\n",
    "    svr_train = r2_score(y_train, predictions_train)\n",
    "    svr_val = r2_score(y_val, predictions_val)\n",
    "    print(\"\\033[1m\" + \"SVR\" + \"\\033[0m\" + \":\")\n",
    "    print('train: ', svr_train*100, '%')\n",
    "    print('val: ', svr_val*100, '%')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
